_BASE_: [
  '../datasets/coco_detection.yml',
  '../runtime.yml',
  #'_base_/optimizer_60e.yml',
  '_base_/optimizer_36e_drop.yml',
  '_base_/dino_r50vd_pan.yml',
  '_base_/yoloe_reader.yml',
]

weights: output/dino_focalnetL_pan_3_0_6_36e_coco/model_final
find_unused_parameters: True
log_iter: 100
snapshot_epoch: 1


pretrain_weights: https://bj.bcebos.com/v1/paddledet/models/pretrained/focalnet_large_lrf_384_fl4_pretrained.pdparams
pretrain_weights: https://bj.bcebos.com/v1/paddledet/models/dino_focalnet_large_fl4_3x_coco.pdparams
# pretrain_weights: https://bj.bcebos.com/v1/paddledet/models/pretrained/focalnet_large_fl4_pretrained_on_o365.pdparams
DETR:
  backbone: FocalNet
  neck: HybridEncoder
  transformer: PPDETRTransformer
  detr_head: DINOHead
  post_process: DETRPostProcess

FocalNet:
  arch: 'focalnet_L_384_22k_fl4'
  out_indices: [1, 2, 3]


HybridEncoder:
  hidden_dim: 256
  use_encoder_idx: [2]
  num_encoder_layers: 6 #1
  encoder_layer:
    name: TransformerLayer
    d_model: 256
    nhead: 8
    dim_feedforward: 2048 #1024
    dropout: 0.
    activation: 'gelu'
  expansion: 1.0


PPDETRTransformer:
  num_queries: 300
  position_embed_type: sine
  feat_strides: [8, 16, 32]
  num_levels: 3
  nhead: 8
  num_decoder_layers: 6
  dim_feedforward: 2048 #1024
  dropout: 0.0
  activation: relu
  num_denoising: 100
  label_noise_ratio: 0.5
  box_noise_scale: 1.0
  learnt_init_query: False
